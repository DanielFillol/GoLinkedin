# LinkedIn Visible Crawler

Um crawler web para LinkedIn que captura perfis visÃ­veis e envia convites de conexÃ£o, com interface web moderna e controle de limites semanais.

## ğŸš€ CaracterÃ­sticas

- **Interface Web Moderna**: UI responsiva com Tailwind CSS, HTMX e Alpine.js
- **Crawler Chromedp**: Motor robusto baseado no projeto anterior
- **Controle de Limites**: Respeita limite de 200 convites/semana por usuÃ¡rio
- **SessÃµes em MemÃ³ria**: Credenciais nÃ£o sÃ£o persistidas em disco
- **Streaming em Tempo Real**: SSE para logs e mÃ©tricas ao vivo
- **Armazenamento CSV**: Dados salvos incrementalmente
- **MultiusuÃ¡rio**: Cada sessÃ£o mantÃ©m suas prÃ³prias credenciais

## ğŸ—ï¸ Arquitetura

```
.
â”œâ”€ cmd/web/           # Servidor web principal
â”œâ”€ internal/
â”‚  â”œâ”€ ui/            # Templates HTML e SSE
â”‚  â”œâ”€ crawler/       # Motor do crawler (chromedp)
â”‚  â”œâ”€ storage/       # Armazenamento CSV e contadores
â”‚  â””â”€ http/          # Handlers e middleware
â”œâ”€ data/             # Dados persistentes (CSV, uploads)
â””â”€ web/static/       # Arquivos estÃ¡ticos (se necessÃ¡rio)
```

## ğŸ“‹ Requisitos

### OpÃ§Ã£o 1: Docker (Recomendado)
- **Docker** e **Docker Compose**
- **Navegador moderno** para a interface web

### OpÃ§Ã£o 2: InstalaÃ§Ã£o Local
- **Go 1.22+**
- **Chrome/Chromium** instalado
- **Navegador moderno** para a interface web

## ğŸ› ï¸ InstalaÃ§Ã£o

### ğŸ³ Com Docker (Recomendado)

1. **Clone o repositÃ³rio**
```bash
git clone <repository-url>
cd linkedin-visible-crawler
```

2. **Execute com Docker Compose**
```bash
# ProduÃ§Ã£o
docker-compose up -d

# Desenvolvimento (com hot reload)
docker-compose --profile dev up -d
```

3. **Acesse a interface**
```
http://localhost:8080
```

### ğŸ”§ InstalaÃ§Ã£o Local

1. **Clone o repositÃ³rio**
```bash
git clone <repository-url>
cd linkedin-visible-crawler
```

2. **Instale as dependÃªncias**
```bash
go mod tidy
```

3. **Execute o servidor**
```bash
make run
# ou
go run ./cmd/web
```

4. **Acesse a interface**
```
http://localhost:8080
```

## ğŸ¯ Como Usar

### 1. Configurar Credenciais
- Acesse o primeiro card "ğŸ” Credenciais do LinkedIn"
- Digite seu email e senha do LinkedIn
- Clique em "Usar nesta sessÃ£o"

### 2. Configurar Queries
- **OpÃ§Ã£o A**: FaÃ§a upload de arquivo .txt (uma query por linha)
- **OpÃ§Ã£o B**: Cole as queries diretamente na textarea
- Exemplo de queries:
  ```
  grupo boticÃ¡rio vendas
  startup tecnologia
  consultor financeiro
  ```

### 3. Executar Crawler
- Configure limites (max cards, max convites por pÃ¡gina)
- Clique em "Iniciar Crawler"
- **Importante**: Aguarde 8 segundos para 2FA manual

### 4. Acompanhar Progresso
- **Status ao Vivo**: Contadores e barra de progresso
- **Logs em Tempo Real**: Acompanhe cada aÃ§Ã£o do crawler
- **Tabela de Convites**: Veja todos os convites enviados

## ğŸ“Š Controles e Limites

### Limite Semanal
- **200 convites por usuÃ¡rio por semana**
- Barra de progresso muda de cor conforme aproxima do limite
- BotÃ£o "Iniciar Crawler" Ã© desabilitado quando limite Ã© atingido

### ConfiguraÃ§Ãµes por PÃ¡gina
- **Max Cards**: Quantos perfis capturar por pÃ¡gina (padrÃ£o: 60)
- **Max Connects**: Quantos convites tentar por pÃ¡gina (padrÃ£o: 3)

## ğŸ”’ SeguranÃ§a

- **Credenciais em MemÃ³ria**: Senhas nunca sÃ£o salvas em disco
- **SessÃµes Ãšnicas**: Cada usuÃ¡rio tem sua prÃ³pria sessÃ£o
- **Limpeza AutomÃ¡tica**: SessÃµes expiradas sÃ£o removidas a cada hora
- **SanitizaÃ§Ã£o**: Todo output HTML Ã© escapado automaticamente

## âš ï¸ Avisos Importantes

### Termos de ServiÃ§o (ToS)
- **Este Ã© um crawler experimental**
- **Use com responsabilidade**
- **Respeite os ToS do LinkedIn**
- **NÃ£o abuse da plataforma**

### Riscos de Bloqueio
- **Limite de 200 convites/semana** Ã© uma diretriz do LinkedIn
- **Pausas entre convites** sÃ£o recomendadas
- **Modo visÃ­vel** (nÃ£o headless) Ã© mais seguro
- **Monitore logs** para detectar bloqueios

### RecomendaÃ§Ãµes
- Use pausas entre execuÃ§Ãµes
- NÃ£o execute 24/7
- Monitore mÃ©tricas de sucesso
- Tenha backup de dados importantes

## ğŸ“ Estrutura de Dados

### Arquivos CSV
```
data/invites.csv
â”œâ”€ timestamp
â”œâ”€ user_email
â”œâ”€ profile_name
â”œâ”€ profile_title
â”œâ”€ company
â”œâ”€ location
â”œâ”€ linkedin_url
â””â”€ query
```

### Uploads
```
data/uploads/queries/
â””â”€ <uuid>.txt          # Arquivos de queries temporÃ¡rios
```

## ğŸ³ Comandos Docker

### ProduÃ§Ã£o
```bash
# Construir e executar
docker-compose up -d

# Ver logs
docker-compose logs -f

# Parar
docker-compose down

# Reconstruir
docker-compose up -d --build
```

### Desenvolvimento
```bash
# Executar com hot reload
docker-compose --profile dev up -d

# Ver logs de desenvolvimento
docker-compose --profile dev logs -f

# Parar desenvolvimento
docker-compose --profile dev down
```

### Comandos Ãšteis
```bash
# Entrar no container
docker-compose exec linkedin-crawler-web sh

# Ver status dos containers
docker-compose ps

# Limpar volumes
docker-compose down -v
```

## ğŸš€ Comandos DisponÃ­veis

### Docker
```bash
docker-compose up -d          # Executar em produÃ§Ã£o
docker-compose --profile dev up -d  # Executar em desenvolvimento
docker-compose down           # Parar containers
docker-compose logs -f        # Ver logs
```

### Local
```bash
make run       # Executar servidor
make build     # Construir binÃ¡rio
make dev       # Modo desenvolvimento
make clean     # Limpar arquivos
make fmt       # Formatar cÃ³digo
make tidy      # Organizar dependÃªncias
make help      # Ver todos os comandos
```

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente
```bash
PORT=8080                    # Porta do servidor
CHROME_HEADLESS=false        # Modo headless do Chrome
CHROME_USER_AGENT=...        # User agent personalizado
```

### Arquivo .env
```bash
cp .env.example .env
# Edite conforme necessÃ¡rio
```

## ğŸ› Troubleshooting

### Problemas Docker

1. **Container nÃ£o inicia**
   ```bash
   # Ver logs
   docker-compose logs
   
   # Reconstruir imagem
   docker-compose up -d --build
   ```

2. **Erro de permissÃ£o no volume**
   ```bash
   # Verificar permissÃµes do diretÃ³rio data/
   ls -la data/
   
   # Corrigir permissÃµes
   chmod -R 755 data/
   ```

3. **Porta jÃ¡ em uso**
   ```bash
   # Verificar processos na porta 8080
   lsof -i :8080
   
   # Parar containers
   docker-compose down
   ```

### Problemas Comuns

1. **Chrome nÃ£o encontrado (Local)**
   - Instale Chrome/Chromium
   - Verifique PATH do sistema

2. **Erro de permissÃ£o**
   - Verifique permissÃµes do diretÃ³rio `data/`
   - Execute com privilÃ©gios adequados

3. **Limite semanal atingido**
   - Aguarde atÃ© segunda-feira
   - Use conta diferente se necessÃ¡rio

4. **Crawler trava**
   - Verifique logs em tempo real
   - Reinicie o servidor se necessÃ¡rio

### Logs
- **Console**: Logs do servidor
- **Interface Web**: Logs em tempo real via SSE
- **Arquivos**: Dados salvos em CSV

## ğŸ“ˆ Monitoramento

### MÃ©tricas em Tempo Real
- Contatos capturados na sessÃ£o
- Convites enviados na semana
- Progresso do limite semanal
- Status de execuÃ§Ã£o

### ExportaÃ§Ã£o de Dados
- **CSV**: Download completo de convites
- **Filtros**: Por usuÃ¡rio, data, query
- **PaginaÃ§Ã£o**: NavegaÃ§Ã£o por resultados

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto Ã© para fins educacionais e experimentais. Use com responsabilidade.

## âš¡ Performance

- **Crawler**: Executa em goroutine separada
- **UI**: AtualizaÃ§Ãµes em tempo real via SSE
- **Storage**: Append-only CSV para performance
- **SessÃµes**: Limpeza automÃ¡tica de memÃ³ria

## ğŸ”„ AtualizaÃ§Ãµes

- **Limpeza automÃ¡tica** de sessÃµes expiradas
- **Contadores semanais** resetam automaticamente
- **Logs em tempo real** para debugging
- **MÃ©tricas persistentes** via CSV

---

**âš ï¸ Lembre-se**: Este Ã© um crawler experimental. Use com responsabilidade e sempre respeite os termos de serviÃ§o do LinkedIn.
